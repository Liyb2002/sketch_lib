#!/usr/bin/env python3
"""
reconstruct_3d_vggt.py - Runs the VGGT model for 3D reconstruction 
from a set of realistic views generated by Zero123++.

Input:  sketch/views_realistic/ (Contains view_0.png, view_1.png, etc.)
Output: sketch/3d_reconstruction/
    1. fused_model.ply (3D Point Cloud)
    2. *_cam.json (Inferred Camera Extrinsics/Intrinsics)
    3. *_verification_render.png (Verification Renders)
    4. *** NEW: *_depth_raw.npy (Raw float depth map) ***
    5. *** NEW: *_depth_vis.png (Visualized 8-bit depth map) ***
"""
import os
import sys
import json
import numpy as np
import torch
import cv2
import open3d as o3d
from PIL import Image
from pathlib import Path

# -----------------------------------------------------------------------------
# 1. SETUP & IMPORTS
# -----------------------------------------------------------------------------
THIS_DIR = os.path.dirname(os.path.abspath(__file__))
VGGT_REPO_ROOT = os.path.join(THIS_DIR, "vggt")
sys.path.insert(0, VGGT_REPO_ROOT)

# Import VGGT modules
try:
    from vggt.models.vggt import VGGT
    from vggt.utils.load_fn import load_and_preprocess_images
    from vggt.utils.pose_enc import pose_encoding_to_extri_intri
    from vggt.utils.geometry import unproject_depth_map_to_point_map
except ImportError:
    print("Error: Could not import VGGT. Make sure the 'vggt' folder is present and accessible.")
    sys.exit(1)

# Configuration based on user request
# Use pathlib for clean path manipulation
ROOT_SKETCH_FOLDER = Path(THIS_DIR) / "sketch"

# 1. INPUT PATH (Views generated from input_realistic.png)
INPUT_VIEWS_DIR = ROOT_SKETCH_FOLDER / "views" 

# 2. OUTPUT PATH
OUTPUT_DIR = ROOT_SKETCH_FOLDER / "3d_reconstruction" 

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
BG_THRESHOLD = 0.95  # Brightness threshold for mask (assumes white background)
MAX_POINTS = 500_000

# -----------------------------------------------------------------------------
# 2. HELPERS
# -----------------------------------------------------------------------------
def normalize_depth_tensor_to_nhw(depth_tensor):
    """Converts torch depth tensor to (N, H, W) numpy array."""
    d = depth_tensor.detach().cpu().numpy()
    if d.ndim == 4 and d.shape[1] == 1:
        d = d[:, 0]
    elif d.ndim == 4 and d.shape[-1] == 1:
        d = d[..., 0]
    return d

def build_object_masks(view_paths, target_h, target_w):
    """
    Creates a boolean mask where True = Object, False = White Background.
    Uses BG_THRESHOLD (0.95 luminance) to aggressively mask the background.
    """
    masks = []
    for p in view_paths:
        img = Image.open(p).convert("RGB")
        img = img.resize((target_w, target_h), Image.Resampling.BILINEAR)
        rgb = np.array(img).astype(np.float32) / 255.0
        gray = rgb.mean(axis=2)
        # Pixel is part of the object if its luminance is BELOW the threshold
        mask = gray < BG_THRESHOLD
        masks.append(mask)
    return np.stack(masks, axis=0)

def render_verification(points, colors, w2c_4x4, K, H, W, save_path):
    """
    Renders the point cloud back to the camera view to verify alignment.

    Assumes:
      - points are in WORLD coordinates (N, 3)
      - w2c_4x4 is WORLD-to-CAMERA extrinsic (4x4)
      - K is intrinsics (3x3)
    """
    # 1. Transform Points to Camera Space
    ones = np.ones((len(points), 1), dtype=np.float32)
    pts_hom = np.hstack([points.astype(np.float32), ones])  # (N,4)
    pts_cam = (w2c_4x4 @ pts_hom.T).T  # (N,4)

    x, y, z = pts_cam[:, 0], pts_cam[:, 1], pts_cam[:, 2]

    # 2. Filter points behind camera
    valid_z = z > 0.01
    if not np.any(valid_z):
        print(f"[WARN] No valid points after z>0 filter for {save_path.name}")
        canvas = np.full((H, W, 3), 255, dtype=np.uint8)
        cv2.imwrite(str(save_path), cv2.cvtColor(canvas, cv2.COLOR_RGB2BGR))
        return

    x, y, z = x[valid_z], y[valid_z], z[valid_z]
    cur_colors = colors[valid_z]

    # 3. Project to Pixels (OpenCV-style)
    fx, fy = K[0, 0], K[1, 1]
    cx, cy = K[0, 2], K[1, 2]

    u = (x * fx / z) + cx
    v = (y * fy / z) + cy

    u = np.round(u).astype(int)
    v = np.round(v).astype(int)

    # 4. Filter valid pixels
    valid_px = (u >= 0) & (u < W) & (v >= 0) & (v < H)
    if not np.any(valid_px):
        print(f"[WARN] No valid pixels inside frame for {save_path.name}")
        canvas = np.full((H, W, 3), 255, dtype=np.uint8)
        cv2.imwrite(str(save_path), cv2.cvtColor(canvas, cv2.COLOR_RGB2BGR))
        return

    u, v, z = u[valid_px], v[valid_px], z[valid_px]
    cur_colors = cur_colors[valid_px]

    # 5. Render (Z-Buffer / Painter's Algorithm)
    # Use WHITE background so it visually matches sketches
    canvas = np.full((H, W, 3), 255, dtype=np.uint8)

    # Sort by depth DESC (furthest first), so nearer points overwrite
    sort_idx = np.argsort(-z)
    uu = u[sort_idx]
    vv = v[sort_idx]
    cc = (cur_colors[sort_idx] * 255).astype(np.uint8)

    canvas[vv, uu] = cc

    # Save
    # We use cv2.imwrite, which needs the path to be a string
    cv2.imwrite(str(save_path), cv2.cvtColor(canvas, cv2.COLOR_RGB2BGR))


def save_depth_map_vis(depth_array, path):
    """
    Normalizes a depth map (H, W) to 8-bit grayscale for visual saving.
    Clips and normalizes to [0, 255].
    Saves a PNG image where depth values are visually represented.
    """
    # 1. Mask out zero (background) values
    mask = depth_array > 0.0

    # 2. Normalize valid depths
    if np.any(mask):
        valid_depths = depth_array[mask]
        
        # Clip to remove outliers (e.g., top 1% max depth) for better contrast
        min_d = valid_depths.min()
        max_d = np.percentile(valid_depths, 99.0)

        # Normalize to [0, 1]
        normalized = np.clip(valid_depths, min_d, max_d)
        normalized = (normalized - min_d) / (max_d - min_d + 1e-6)
        
        # Create 8-bit image
        # Nearer = Brighter (often preferred for visualization)
        canvas = np.zeros_like(depth_array, dtype=np.uint8)
        canvas[mask] = ((1.0 - normalized) * 255).astype(np.uint8)
    else:
        # If no valid depth, save a black image
        canvas = np.zeros_like(depth_array, dtype=np.uint8)

    # Save PNG
    Image.fromarray(canvas).save(path)


# -----------------------------------------------------------------------------
# 3. MAIN PIPELINE
# -----------------------------------------------------------------------------
def main():
    if not OUTPUT_DIR.exists():
        OUTPUT_DIR.mkdir(parents=True)
    
    # 1. Load Images
    # Use pathlib to glob for image files
    view_paths = sorted(list(INPUT_VIEWS_DIR.glob('*.png')))
    
    if not view_paths:
        print(f"No images found in {INPUT_VIEWS_DIR}")
        print("Ensure you have run the Zero123++ script successfully.")
        return
    
    print(f"Loading {len(view_paths)} images from {INPUT_VIEWS_DIR.name}...")
    
    # Convert pathlib paths to strings for the legacy VGGT loading function
    images = load_and_preprocess_images([str(p) for p in view_paths]).to(DEVICE)  # (N, 3, H, W)
    H, W = images.shape[-2:]
    print(f"Model input size: H={H}, W={W}")
    

    # 2. Run VGGT
    print("Running VGGT Model...")
    model = VGGT.from_pretrained("facebook/VGGT-1B").to(DEVICE)
    model.eval()

    with torch.no_grad():
        images_batch = images.unsqueeze(0)  # (1, N, 3, H, W)
        tokens, idx = model.aggregator(images_batch)
        
        # Camera Head
        pose_enc = model.camera_head(tokens)[-1]
        extrinsics, intrinsics = pose_encoding_to_extri_intri(pose_enc, (H, W))
        
        # Depth Head
        depth_tensor, _ = model.depth_head(tokens, images_batch, idx)

    # Convert to Numpy
    extrinsics_np = extrinsics.squeeze(0).cpu().numpy()  # (N, 3, 4) W2C
    intrinsics_np = intrinsics.squeeze(0).cpu().numpy()  # (N, 3, 3)
    depth_np = normalize_depth_tensor_to_nhw(depth_tensor.squeeze(0))  # (N, H, W)

    print("Extrinsics shape:", extrinsics_np.shape)
    print("Intrinsics shape:", intrinsics_np.shape)
    print("Depth shape:     ", depth_np.shape)

    # --- NEW: Save Depth Maps ---
    print("\nSaving Raw and Visualized Depth Maps...")
    for i, vp in enumerate(view_paths):
        name = vp.stem
        
        # 1. Save Raw Float Depth (.npy)
        raw_depth_path = OUTPUT_DIR / f"{name}_depth_raw.npy"
        np.save(raw_depth_path, depth_np[i])

        # 2. Save Visualized Depth (.png)
        vis_depth_path = OUTPUT_DIR / f"{name}_depth_vis.png"
        save_depth_map_vis(depth_np[i], vis_depth_path)
    
        print(f"  - View {i}: Saved {raw_depth_path.name} and {vis_depth_path.name}")
    # ----------------------------

    # 3. Generate Point Cloud
    print("\nGenerating Point Cloud...")
    masks = build_object_masks([str(p) for p in view_paths], H, W)
    depth_masked = depth_np.copy()
    
    # Apply mask: set depth to 0.0 where background is white
    depth_masked[~masks] = 0.0

    depth_for_unproj = depth_masked[..., None]  # (N, H, W, 1)

    point_map = unproject_depth_map_to_point_map(
        depth_for_unproj,
        extrinsics_np,
        intrinsics_np
    )  # (N, H, W, 3)
    
    # Flatten
    points_all = point_map.reshape(-1, 3)

    # Get Colors for PLY (per-view, resized to H,W)
    colors_list = []
    for p in view_paths:
        img = Image.open(p).convert("RGB").resize((W, H))
        colors_list.append(np.array(img).reshape(-1, 3) / 255.0)
    colors_all = np.concatenate(colors_list, axis=0)

    # Filter Valid Points
    valid_mask = np.isfinite(points_all).all(axis=1) & (np.abs(points_all).sum(axis=1) > 0)
    points_final = points_all[valid_mask]
    colors_final = colors_all[valid_mask]

    print(f"Total raw points:    {points_all.shape[0]}")
    print(f"Valid nonzero points:{points_final.shape[0]}")

    # Downsample if needed
    if len(points_final) > MAX_POINTS:
        idx_ds = np.random.choice(len(points_final), MAX_POINTS, replace=False)
        points_final = points_final[idx_ds]
        colors_final = colors_final[idx_ds]
        print(f"Downsampled to {MAX_POINTS} points")

    # Save PLY (3D Shape)
    ply_path = OUTPUT_DIR / "fused_model.ply"
    pcd = o3d.geometry.PointCloud()
    pcd.points = o3d.utility.Vector3dVector(points_final)
    pcd.colors = o3d.utility.Vector3dVector(colors_final)
    o3d.io.write_point_cloud(str(ply_path), pcd)
    print(f"Saved fused 3D point cloud: {ply_path}")

    # 4. Save Cameras & Render Verification
    print("\nSaving Cameras and Verifying Alignment (W2C)...")
    
    for i, vp in enumerate(view_paths):
        # name is 'view_0', 'view_1', etc.
        name = vp.stem 

        # W2C extrinsic from VGGT for this view
        w2c_3x4 = extrinsics_np[i]  # (3,4)
        K = intrinsics_np[i]        # (3,3)

        # Make full 4x4 W2C matrix
        w2c_4x4 = np.eye(4, dtype=np.float32)
        w2c_4x4[:3, :4] = w2c_3x4

        # Save JSON (Inferred Camera Positions)
        cam_data = {
            "extrinsics_w2c": w2c_4x4.tolist(),
            "intrinsics": K.tolist()
        }
        json_path = OUTPUT_DIR / f"{name}_cam.json"
        with open(json_path, 'w') as f:
            json.dump(cam_data, f, indent=4)

        # Render verification (using W2C directly)
        render_path = OUTPUT_DIR / f"{name}_verification_render.png"
        render_verification(points_final, colors_final, w2c_4x4, K, H, W, render_path)

        print(f"  - View {i}:")
        print(f"      JSON:   {json_path.name}")
        print(f"      Render: {render_path.name}")

    print(f"\nSuccess! Check {OUTPUT_DIR} for:")
    print("  - fused_model.ply (3D Shape)")
    print("  - *_cam.json (Inferred Camera Positions)")
    print("  - *_verification_render.png (Verification Renders)")
    print("  - *_depth_raw.npy (Raw Depth Maps)")
    print("  - *_depth_vis.png (Visual Depth Maps)")

if __name__ == "__main__":
    main()