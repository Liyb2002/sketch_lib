#!/usr/bin/env python3
"""
reconstruct_3d_vggt.py - Runs the VGGT model for 3D reconstruction
from a set of realistic views generated by Zero123++.

Input:  sketch/views_realistic/ (Contains view_0.png, view_1.png, etc.)
Output: sketch/3d_reconstruction/
    1. fused_model.ply (3D Point Cloud)
    2. *_cam.json (Inferred Camera Extrinsics/Intrinsics)
    3. *_verification_render.png (Verification Renders)
    4. *** NEW: *_overlay.png (Input + Render overlay) ***

NOTE: Depth maps are NOT saved in this version.
"""
import os
import sys
import json
import numpy as np
import torch
import cv2
import open3d as o3d
from PIL import Image
from pathlib import Path

# -----------------------------------------------------------------------------
# 1. SETUP & IMPORTS
# -----------------------------------------------------------------------------
THIS_DIR = os.path.dirname(os.path.abspath(__file__))
VGGT_REPO_ROOT = os.path.join(THIS_DIR, "vggt")
sys.path.insert(0, VGGT_REPO_ROOT)

# Import VGGT modules
try:
    from vggt.models.vggt import VGGT
    from vggt.utils.load_fn import load_and_preprocess_images
    from vggt.utils.pose_enc import pose_encoding_to_extri_intri
    from vggt.utils.geometry import unproject_depth_map_to_point_map
except ImportError:
    print("Error: Could not import VGGT. Make sure the 'vggt' folder is present and accessible.")
    sys.exit(1)

ROOT_SKETCH_FOLDER = Path(THIS_DIR) / "sketch"
INPUT_VIEWS_DIR = ROOT_SKETCH_FOLDER / "views"
OUTPUT_DIR = ROOT_SKETCH_FOLDER / "3d_reconstruction"

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
BG_THRESHOLD = 0.95  # Brightness threshold for mask (assumes white background)
MAX_POINTS = 500_000

# -----------------------------------------------------------------------------
# 2. HELPERS
# -----------------------------------------------------------------------------
def normalize_depth_tensor_to_nhw(depth_tensor):
    """Converts torch depth tensor to (N, H, W) numpy array."""
    d = depth_tensor.detach().cpu().numpy()
    if d.ndim == 4 and d.shape[1] == 1:
        d = d[:, 0]
    elif d.ndim == 4 and d.shape[-1] == 1:
        d = d[..., 0]
    return d

def build_object_masks(view_paths, target_h, target_w):
    """
    Creates a boolean mask where True = Object, False = White Background.
    Uses BG_THRESHOLD (0.95 luminance) to aggressively mask the background.
    """
    masks = []
    for p in view_paths:
        img = Image.open(p).convert("RGB")
        img = img.resize((target_w, target_h), Image.Resampling.BILINEAR)
        rgb = np.array(img).astype(np.float32) / 255.0
        gray = rgb.mean(axis=2)
        mask = gray < BG_THRESHOLD
        masks.append(mask)
    return np.stack(masks, axis=0)

def render_verification(points, colors, w2c_4x4, K, H, W, save_path):
    """
    Renders the point cloud back to the camera view to verify alignment.

    Assumes:
      - points are in WORLD coordinates (N, 3)
      - w2c_4x4 is WORLD-to-CAMERA extrinsic (4x4)
      - K is intrinsics (3x3)
    """
    ones = np.ones((len(points), 1), dtype=np.float32)
    pts_hom = np.hstack([points.astype(np.float32), ones])  # (N,4)
    pts_cam = (w2c_4x4 @ pts_hom.T).T  # (N,4)

    x, y, z = pts_cam[:, 0], pts_cam[:, 1], pts_cam[:, 2]

    valid_z = z > 0.01
    if not np.any(valid_z):
        print(f"[WARN] No valid points after z>0 filter for {save_path.name}")
        canvas = np.full((H, W, 3), 255, dtype=np.uint8)
        cv2.imwrite(str(save_path), cv2.cvtColor(canvas, cv2.COLOR_RGB2BGR))
        return

    x, y, z = x[valid_z], y[valid_z], z[valid_z]
    cur_colors = colors[valid_z]

    fx, fy = K[0, 0], K[1, 1]
    cx, cy = K[0, 2], K[1, 2]

    u = (x * fx / z) + cx
    v = (y * fy / z) + cy

    u = np.round(u).astype(int)
    v = np.round(v).astype(int)

    valid_px = (u >= 0) & (u < W) & (v >= 0) & (v < H)
    if not np.any(valid_px):
        print(f"[WARN] No valid pixels inside frame for {save_path.name}")
        canvas = np.full((H, W, 3), 255, dtype=np.uint8)
        cv2.imwrite(str(save_path), cv2.cvtColor(canvas, cv2.COLOR_RGB2BGR))
        return

    u, v, z = u[valid_px], v[valid_px], z[valid_px]
    cur_colors = cur_colors[valid_px]

    canvas = np.full((H, W, 3), 255, dtype=np.uint8)

    # Painter's algorithm: furthest first, nearest last
    sort_idx = np.argsort(-z)
    uu = u[sort_idx]
    vv = v[sort_idx]
    cc = (cur_colors[sort_idx] * 255).astype(np.uint8)

    canvas[vv, uu] = cc
    cv2.imwrite(str(save_path), cv2.cvtColor(canvas, cv2.COLOR_RGB2BGR))

def save_overlay(input_img_path, render_img_path, overlay_out_path, alpha=0.55):
    """
    Saves an overlay image of render on top of input.
    alpha = weight of input; (1-alpha) = weight of render.
    """
    inp = Image.open(input_img_path).convert("RGB")
    rnd = Image.open(render_img_path).convert("RGB")

    # Ensure same size
    if rnd.size != inp.size:
        rnd = rnd.resize(inp.size, Image.Resampling.BILINEAR)

    inp_np = np.array(inp).astype(np.float32)
    rnd_np = np.array(rnd).astype(np.float32)

    overlay = alpha * inp_np + (1.0 - alpha) * rnd_np
    overlay = np.clip(overlay, 0, 255).astype(np.uint8)
    Image.fromarray(overlay).save(overlay_out_path)

# -----------------------------------------------------------------------------
# 3. MAIN PIPELINE
# -----------------------------------------------------------------------------
def main():
    if not OUTPUT_DIR.exists():
        OUTPUT_DIR.mkdir(parents=True)

    view_paths = sorted(list(INPUT_VIEWS_DIR.glob("*.png")))
    if not view_paths:
        print(f"No images found in {INPUT_VIEWS_DIR}")
        print("Ensure you have run the Zero123++ script successfully.")
        return

    print(f"Loading {len(view_paths)} images from {INPUT_VIEWS_DIR.name}...")
    images = load_and_preprocess_images([str(p) for p in view_paths]).to(DEVICE)  # (N, 3, H, W)
    H, W = images.shape[-2:]
    print(f"Model input size: H={H}, W={W}")

    print("Running VGGT Model...")
    model = VGGT.from_pretrained("facebook/VGGT-1B").to(DEVICE)
    model.eval()

    with torch.no_grad():
        images_batch = images.unsqueeze(0)  # (1, N, 3, H, W)
        tokens, idx = model.aggregator(images_batch)

        pose_enc = model.camera_head(tokens)[-1]
        extrinsics, intrinsics = pose_encoding_to_extri_intri(pose_enc, (H, W))

        depth_tensor, _ = model.depth_head(tokens, images_batch, idx)

    extrinsics_np = extrinsics.squeeze(0).cpu().numpy()  # (N, 3, 4) W2C
    intrinsics_np = intrinsics.squeeze(0).cpu().numpy()  # (N, 3, 3)
    depth_np = normalize_depth_tensor_to_nhw(depth_tensor.squeeze(0))  # (N, H, W)

    print("Extrinsics shape:", extrinsics_np.shape)
    print("Intrinsics shape:", intrinsics_np.shape)
    print("Depth shape:     ", depth_np.shape)

    print("\nGenerating Point Cloud...")
    masks = build_object_masks([str(p) for p in view_paths], H, W)
    depth_masked = depth_np.copy()
    depth_masked[~masks] = 0.0
    depth_for_unproj = depth_masked[..., None]  # (N, H, W, 1)

    point_map = unproject_depth_map_to_point_map(
        depth_for_unproj,
        extrinsics_np,
        intrinsics_np
    )  # (N, H, W, 3)

    points_all = point_map.reshape(-1, 3)

    colors_list = []
    for p in view_paths:
        img = Image.open(p).convert("RGB").resize((W, H))
        colors_list.append(np.array(img).reshape(-1, 3) / 255.0)
    colors_all = np.concatenate(colors_list, axis=0)

    valid_mask = np.isfinite(points_all).all(axis=1) & (np.abs(points_all).sum(axis=1) > 0)
    points_final = points_all[valid_mask]
    colors_final = colors_all[valid_mask]

    print(f"Total raw points:     {points_all.shape[0]}")
    print(f"Valid nonzero points: {points_final.shape[0]}")

    if len(points_final) > MAX_POINTS:
        idx_ds = np.random.choice(len(points_final), MAX_POINTS, replace=False)
        points_final = points_final[idx_ds]
        colors_final = colors_final[idx_ds]
        print(f"Downsampled to {MAX_POINTS} points")

    ply_path = OUTPUT_DIR / "fused_model.ply"
    pcd = o3d.geometry.PointCloud()
    pcd.points = o3d.utility.Vector3dVector(points_final)
    pcd.colors = o3d.utility.Vector3dVector(colors_final)
    o3d.io.write_point_cloud(str(ply_path), pcd)
    print(f"Saved fused 3D point cloud: {ply_path}")

    print("\nSaving Cameras and Verifying Alignment (W2C)...")
    for i, vp in enumerate(view_paths):
        name = vp.stem

        w2c_3x4 = extrinsics_np[i]
        K = intrinsics_np[i]

        w2c_4x4 = np.eye(4, dtype=np.float32)
        w2c_4x4[:3, :4] = w2c_3x4

        cam_data = {"extrinsics_w2c": w2c_4x4.tolist(), "intrinsics": K.tolist()}
        json_path = OUTPUT_DIR / f"{name}_cam.json"
        with open(json_path, "w") as f:
            json.dump(cam_data, f, indent=4)

        render_path = OUTPUT_DIR / f"{name}_verification_render.png"
        render_verification(points_final, colors_final, w2c_4x4, K, H, W, render_path)

        # NEW: overlay render on top of the input image
        overlay_path = OUTPUT_DIR / f"{name}_overlay.png"
        save_overlay(vp, render_path, overlay_path, alpha=0.55)

        print(f"  - View {i}:")
        print(f"      JSON:    {json_path.name}")
        print(f"      Render:  {render_path.name}")
        print(f"      Overlay: {overlay_path.name}")

    print(f"\nSuccess! Check {OUTPUT_DIR} for:")
    print("  - fused_model.ply (3D Shape)")
    print("  - *_cam.json (Inferred Camera Positions)")
    print("  - *_verification_render.png (Verification Renders)")
    print("  - *_overlay.png (Input + Render overlay)")

if __name__ == "__main__":
    main()
